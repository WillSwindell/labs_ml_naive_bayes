{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Lab\n",
    "\n",
    "Will Swindell 2022\n",
    "\n",
    "Experimentation with vectorization of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to home directory (if not already there)\n",
    "if(os.getcwd() == '/Users/willswindell/Documents/CS 370 ML/Datasets/movies_reviews/pos'):\n",
    "    os.chdir('../../../Lab 5/labs_ml_naive_bayes')\n",
    "    \n",
    "if(os.getcwd() == '/Users/willswindell/Documents/CS 370 ML/Datasets/movies_reviews/neg'):\n",
    "    os.chdir('../../../Lab 5/labs_ml_naive_bayes')\n",
    "\n",
    "# Folder Paths\n",
    "path_pos = \"../../Datasets/movies_reviews/pos\"\n",
    "path_neg = \"../../Datasets/movies_reviews/neg\"\n",
    "\n",
    "# iterate through and store all pos files\n",
    "pos_files = []\n",
    "os.chdir(path_pos)\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path_pos}/{file}\"\n",
    "  \n",
    "        # add to list of positives\n",
    "        pos_files.append(file_path)\n",
    "\n",
    "# Change back to home directory\n",
    "os.chdir('../../../Lab 5/labs_ml_naive_bayes')\n",
    "        \n",
    "# iterate through all neg files\n",
    "neg_files = []\n",
    "os.chdir(path_neg)\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{path_neg}/{file}\"\n",
    "  \n",
    "        # add to list of negatives\n",
    "        neg_files.append(file_path)\n",
    "\n",
    "        \n",
    "# Change back to home directory\n",
    "os.chdir('../../../Lab 5/labs_ml_naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each file and store its content\n",
    "\n",
    "def readFiles(files):\n",
    "    contents = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            contents.append(f.read())\n",
    "    return contents\n",
    "\n",
    "pos_content = readFiles(pos_files)\n",
    "neg_content = readFiles(neg_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for vect\n",
    "\n",
    "# Returns dictionary of word counts for a text\n",
    "def get_word_counts(text, all_words):\n",
    "    wc={}\n",
    "    words = get_words(text)\n",
    "    # Loop over all the entries\n",
    "\n",
    "    for word in words:\n",
    "        if (word not in stopwords) and (word in all_words):\n",
    "            wc[word] = wc.get(word,0)+1\n",
    "\n",
    "    return wc\n",
    "\n",
    "# splits text into words\n",
    "def get_words(txt):\n",
    "    # Split words by all non-alpha characters\n",
    "    words=re.compile(r'[^A-Z^a-z]+').split(txt)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    return [word.lower() for word in words if word!='']\n",
    "\n",
    "\n",
    "# converts counts into a vector\n",
    "def get_word_vector(word_list, wc):\n",
    "    v = [0]*len(word_list)\n",
    "    for i in range(len(word_list)):\n",
    "        if word_list[i] in wc:\n",
    "            v[i] = wc[word_list[i]]\n",
    "    return v\n",
    "\n",
    "\n",
    "# prints matrix\n",
    "def print_word_matrix(docs):\n",
    "    for d in docs:\n",
    "        print (d[0], d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost']\n"
     ]
    }
   ],
   "source": [
    "# For stop word removal\n",
    "\n",
    "stop_words_file = \"stop_words.txt\"\n",
    "f = open(stop_words_file, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "stopwords = []\n",
    "for line in f:\n",
    "    stopwords.append(line.strip())\n",
    "    \n",
    "f.close()\n",
    "\n",
    "print(stopwords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read reviews and create word vectors\n",
    "\n",
    "all_words = {}\n",
    "doc_id = 1\n",
    "vectors = []\n",
    "\n",
    "for review in pos_content:\n",
    "    doc_words = get_words(review)\n",
    "    for w in doc_words :\n",
    "        if w not in stopwords:\n",
    "            all_words[w] = all_words.get(w,0)+1\n",
    "            \n",
    "for review in neg_content:\n",
    "    doc_words = get_words(review)\n",
    "    for w in doc_words :\n",
    "        if w not in stopwords:\n",
    "            all_words[w] = all_words.get(w,0)+1\n",
    "            \n",
    "unique_words = set()\n",
    "for w, count in all_words.items():\n",
    "    if all_words[w] > 1 :\n",
    "        unique_words.add(w)\n",
    "        \n",
    "for review in pos_content:\n",
    "    vectors.append([\"d\"+str(doc_id), get_word_counts(review,unique_words)])\n",
    "    doc_id += 1\n",
    "    \n",
    "for review in neg_content:\n",
    "    vectors.append([\"d\"+str(doc_id), get_word_counts(review,unique_words)])\n",
    "    doc_id += 1\n",
    "\n",
    "unique_words=list(unique_words)\n",
    "#print(\"All unique words:\",unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005\n",
      "2005\n"
     ]
    }
   ],
   "source": [
    "# Create list of classifications: 1 is positive, 2 is negative\n",
    "\n",
    "print(len(vectors))\n",
    "Y = np.concatenate((np.ones(1000), np.zeros(1005)))\n",
    "print(len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to vector file\n",
    "\n",
    "out = open(\"reviews_vectors.txt\", \"w\")\n",
    "\n",
    "# write a header which contains the words themselves\n",
    "for w in unique_words:\n",
    "    out.write('\\t' + w)\n",
    "out.write('\\n')\n",
    "\n",
    "# print_word_matrix to file\n",
    "for i in range(len(vectors)):\n",
    "    vector = get_word_vector(unique_words, vectors[i][1])\n",
    "    out.write(vectors[i][0])\n",
    "    for x in vector:\n",
    "        out.write('\\t' + str(x))\n",
    "    out.write('\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vector file\n",
    "\n",
    "reviews_vectors_file = \"reviews_vectors.txt\"\n",
    "f = open(reviews_vectors_file, \"r\", encoding=\"utf-8\")\n",
    "s = f.read()\n",
    "\n",
    "def read_vector_file(file_name):\n",
    "    f = open(file_name)\n",
    "    lines=[line for line in f]\n",
    "  \n",
    "    # First line is the column headers\n",
    "    colnames=lines[0].strip().split('\\t')[:]\n",
    "    # print(colnames)\n",
    "    rownames=[]\n",
    "    data=[]\n",
    "    for line in lines[1:]:\n",
    "        p=line.strip().split('\\t')\n",
    "        # First column in each row is the rowname\n",
    "        if len(p)>1:\n",
    "            rownames.append(p[0])\n",
    "            # The data for this row is the remainder of the row\n",
    "            data.append([float(x) for x in p[1:]])\n",
    "    return rownames,colnames,data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read vector file in as data and set up train and test data\n",
    "\n",
    "reviews, words, data = read_vector_file(reviews_vectors_file)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, Y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized train score: 0.9731920199501247\n",
      "Normalized test score: 0.8229426433915212\n"
     ]
    }
   ],
   "source": [
    "# Create and train Naive Bayes model with test and train sets\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf = clf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Normalized train score:\", clf.score(X_train, Y_train))\n",
    "print(\"Normalized test score:\", clf.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Results\n",
    "\n",
    "The above model correctly classifies the test reviews an impressive 82 percent of the time. For the next steps, I have found 5 reviews of new movies currently playing in theaters from rogerebert.com and stored them in a new folder. The next part of this lab predicts the class of these 5 new reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5new/r3.txt', '5new/r2.txt', '5new/r1.txt', '5new/r5.txt', '5new/r4.txt']\n"
     ]
    }
   ],
   "source": [
    "# read in filee names from new reviews\n",
    "\n",
    "test_files = []\n",
    "test_path = '5new'\n",
    "os.chdir(test_path)\n",
    "for file in os.listdir():\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = f\"{test_path}/{file}\"\n",
    "  \n",
    "        # add to list of test reviews\n",
    "        test_files.append(file_path)\n",
    "\n",
    "# Print test files to know order of reading in - r1 and r2 are negative and the rest are positive\n",
    "print(test_files)\n",
    "\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read contents and create word vectors for each review\n",
    "\n",
    "test_content = readFiles(test_files)\n",
    "test_vectors = []\n",
    "            \n",
    "for review in test_content:\n",
    "    test_vectors.append([\"d\"+str(doc_id), get_word_counts(review,unique_words)])        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write vectors to a new vector file, ensuring we use original word. list to retain ordering\n",
    "\n",
    "out = open(\"test_vectors.txt\", \"w\")\n",
    "\n",
    "# write a header which contains the words themselves\n",
    "for w in unique_words:\n",
    "    out.write('\\t' + w)\n",
    "out.write('\\n')\n",
    "\n",
    "# print_word_matrix to file\n",
    "for i in range(len(test_vectors)):\n",
    "    vector = get_word_vector(unique_words, test_vectors[i][1])\n",
    "    out.write(test_vectors[i][0])\n",
    "    for x in vector:\n",
    "        out.write('\\t' + str(x))\n",
    "    out.write('\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in vector  file\n",
    "\n",
    "test_vectors_file = \"test_vectors.txt\"\n",
    "f = open(test_vectors_file, \"r\", encoding=\"utf-8\")\n",
    "s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: [1. 1. 0. 1. 1.]\n",
      "Actual price: [1, 0, 0, 1, 1]\n",
      "Test Data Accuracy: 80.0%\n"
     ]
    }
   ],
   "source": [
    "# Classifty new reviews\n",
    "\n",
    "reviews, words, test_data = read_vector_file(test_vectors_file)\n",
    "\n",
    "# Actual classification\n",
    "test_class = [1, 0, 0, 1, 1]\n",
    "\n",
    "y_predicted = clf.predict(test_data)\n",
    "\n",
    "def accuracy(preds, actual):\n",
    "    denom = len(preds)\n",
    "    correct = 0\n",
    "    for i, pred in enumerate(preds):\n",
    "        if(pred == actual[i]):\n",
    "            correct += 1\n",
    "    return((correct/denom)*100)\n",
    "\n",
    "print(\"Predicted price:\",y_predicted )\n",
    "print(\"Actual price:\",test_class)\n",
    "print(\"Test Data Accuracy: {}%\".format(accuracy(y_predicted, test_class)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Results\n",
    "\n",
    "After running the model on the new reviews, one of the five reviews was classified incorrectly. Looking back at our original training accuracy of 82 percent, we should expect this classifier to misidentify about one review as it is, so this comes as no surprise. It is definitely impressive that this model can correctly classifiy 4/5 new reviews, especially when we consider that these reviews may contain new words (or even actor/director names) which have not been seen before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
